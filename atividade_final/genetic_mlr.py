import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent / 'src'))
from metrics import compute_metrics
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score
import numpy as np
import pygad
from src.logging_utils import get_logger
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import r2_score
import scipy.stats as stats
import optuna
import json
from typing import Dict, Any, Tuple
import warnings
warnings.filterwarnings('ignore', category=optuna.exceptions.ExperimentalWarning)

logger = get_logger('genetic_mlr')

# --- Configura√ß√£o de Hiperpar√¢metros ---
class HyperparameterConfig:
    """Classe para gerenciar configura√ß√£o de hiperpar√¢metros do algoritmo gen√©tico."""
    
    def __init__(self):
        """Inicializa com configura√ß√£o padr√£o."""
        self.default_params = {
            "num_generations": 80,   # Reduzido para velocidade
            "sol_per_pop": 40,       # Reduzido para velocidade
            "K_tournament": 4,
            "keep_parents": 6,       # Reduzido
            "cv_folds": 4,
            "max_features": 70,      # Mais conservador
            "feature_penalty": 0.2
        }
        
        self.search_space = {
            "num_generations": (30, 150),  # Reduzido para velocidade
            "sol_per_pop": (20, 60),       # Reduzido para velocidade
            "K_tournament": (2, 6),
            "keep_parents": (2, 15),
            "cv_folds": (3, 5),            # Reduzido para velocidade
            "max_features": (30, 120),     # Mais conservador
            "feature_penalty": (0.1, 0.4)
        }
        
        self.best_params = None
        self.best_score = None
        
    def get_default_params(self) -> Dict[str, Any]:
        """Retorna par√¢metros padr√£o."""
        return self.default_params.copy()
    
    def get_search_space(self) -> Dict[str, Tuple]:
        """Retorna espa√ßo de busca para otimiza√ß√£o."""
        return self.search_space.copy()
    
    def update_best_params(self, params: Dict[str, Any], score: float):
        """Atualiza os melhores par√¢metros encontrados."""
        self.best_params = params.copy()
        self.best_score = score
        
    def save_best_params(self, filepath: Path):
        """Salva os melhores par√¢metros em arquivo JSON."""
        if self.best_params:
            with open(filepath, 'w') as f:
                json.dump({
                    'best_params': self.best_params,
                    'best_score': self.best_score
                }, f, indent=2)
                
    def load_best_params(self, filepath: Path):
        """Carrega os melhores par√¢metros de arquivo JSON."""
        if filepath.exists():
            with open(filepath, 'r') as f:
                data = json.load(f)
                self.best_params = data.get('best_params')
                self.best_score = data.get('best_score')

# Inst√¢ncia global da configura√ß√£o
hyperparams = HyperparameterConfig()

BASE_DIR = Path(__file__).resolve().parent
CALIBRATION_CSV = BASE_DIR / 'data' / 'csv_new' / 'calibration.csv'
TEST_CSV = BASE_DIR / 'data' / 'csv_new' / 'test.csv'
VALIDATION_CSV = BASE_DIR / 'data' / 'csv_new' / 'validation.csv'
IDRC_CSV = BASE_DIR / 'data' / 'csv_new' / 'idrc_validation.csv'
WL_CSV = BASE_DIR / 'data' / 'csv_new' / 'wl.csv'
OUTPUT_CSV = BASE_DIR / 'results' / 'genetic_results.csv'
OUTPUT_PLOTS = BASE_DIR / 'results' / 'genetic_plots'
OUTPUT_PLOTS.mkdir(parents=True, exist_ok=True)

def load_data(calib_path: Path, idrc_path: Path) -> tuple[pd.DataFrame, pd.DataFrame]:
    """Carrega os dados de calibra√ß√£o e IDRC.

    Args:
        calib_path (Path): Caminho para o CSV de calibra√ß√£o.
        idrc_path (Path): Caminho para o CSV de valida√ß√£o IDRC.

    Returns:
        tuple: DataFrames de calibra√ß√£o e IDRC.
    """
    logger.info(f"Carregando dados de calibra√ß√£o de: {calib_path}")
    df = pd.read_csv(calib_path)
    logger.info(f"Dados de calibra√ß√£o carregados. Shape: {df.shape}")
    logger.info(f"Carregando dados IDRC de: {idrc_path}")
    idrc_df = pd.read_csv(idrc_path)
    logger.info(f"Dados IDRC carregados. Shape: {idrc_df.shape}")
    return df, idrc_df

def prepare_datasets(df: pd.DataFrame, idrc_df: pd.DataFrame):
    """Prepara os conjuntos de treino, teste e valida√ß√£o, com imputa√ß√£o e normaliza√ß√£o.

    Args:
        df (pd.DataFrame): Dados de calibra√ß√£o.
        idrc_df (pd.DataFrame): Dados de valida√ß√£o IDRC.

    Returns:
        tuple: Arrays e listas para treino, teste, valida√ß√£o, nomes das features e scaler.
    """
    # Carrega os dados de teste e valida√ß√£o
    test_df = pd.read_csv(TEST_CSV)
    validation_df = pd.read_csv(VALIDATION_CSV)
    
    # Seleciona apenas as colunas num√©ricas (excluindo a √∫ltima coluna que √© o target)
    feature_columns = [col for col in df.columns if col != 'target']
    
    # Prepara os dados de treino (usando todos os dados de calibra√ß√£o)
    X_train = df[feature_columns]
    y_train = df['target']
    
    # Prepara os dados de teste (do arquivo test.csv)
    X_test = test_df[feature_columns]
    y_test = test_df['target']
    
    # Prepara os dados de valida√ß√£o (do arquivo validation.csv)
    X_val = validation_df[feature_columns]
    y_val = idrc_df['reference']  # Usando a coluna 'reference' do IDRC
    
    # Imputa√ß√£o de dados faltantes
    imputer = SimpleImputer(strategy='mean')
    X_train_imputed = imputer.fit_transform(X_train)
    X_test_imputed = imputer.transform(X_test)
    X_val_imputed = imputer.transform(X_val)
    
    # Normaliza√ß√£o dos dados
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_imputed)
    X_test_scaled = scaler.transform(X_test_imputed)
    X_val_scaled = scaler.transform(X_val_imputed)
    
    return X_train_scaled, y_train, X_test_scaled, y_test, X_val_scaled, y_val, feature_columns, scaler

def custom_mutation_func(offspring, ga_instance):
    """Fun√ß√£o de muta√ß√£o customizada inteligente que preserva boas solu√ß√µes.
    
    Args:
        offspring: Popula√ß√£o de descendentes
        ga_instance: Inst√¢ncia do GA
    
    Returns:
        offspring: Popula√ß√£o mutada
    """
    MAX_FEATURES = getattr(custom_mutation_func, 'max_features', 90)
    
    for chromosome_idx in range(offspring.shape[0]):
        original_solution = offspring[chromosome_idx].copy()
        original_features = np.sum(original_solution)
        
        # üß¨ MUTA√á√ÉO INTELIGENTE ADAPTATIVA:
        # Taxa de muta√ß√£o baseada no n√∫mero de features E contexto evolutivo
        boost_mutation = getattr(custom_mutation_func, 'boost_mutation', False)
        
        if original_features <= 30:
            # Solu√ß√µes com poucas features: muta√ß√£o suave (preservar qualidade)
            mutation_probability = 0.04 if boost_mutation else 0.02
            max_mutations = 5 if boost_mutation else 3
        elif original_features <= 50:
            # Solu√ß√µes medianas: muta√ß√£o moderada
            mutation_probability = 0.10 if boost_mutation else 0.05
            max_mutations = 8 if boost_mutation else 5
        else:
            # Solu√ß√µes com muitas features: muta√ß√£o agressiva (simplificar)
            mutation_probability = 0.25 if boost_mutation else 0.15
            max_mutations = 15 if boost_mutation else 10
        
        mutations_made = 0
        
        # Aplica muta√ß√£o com limite
        for gene_idx in range(offspring.shape[1]):
            if mutations_made >= max_mutations:
                break
                
            if np.random.random() < mutation_probability:
                # Flip do bit
                offspring[chromosome_idx, gene_idx] = 1 - offspring[chromosome_idx, gene_idx]
                mutations_made += 1
        
        # ‚úÖ Controle de restri√ß√µes INTELIGENTE
        selected_count = np.sum(offspring[chromosome_idx])
        
        if selected_count > MAX_FEATURES:
            # Remove features com MENOR correla√ß√£o (manter as melhores)
            selected_indices = np.where(offspring[chromosome_idx] == 1)[0]
            excess_count = selected_count - MAX_FEATURES
            
            # Calcula correla√ß√µes das features selecionadas
            if hasattr(fitness_func, 'X_train') and hasattr(fitness_func, 'y_train'):
                correlations = np.abs(np.corrcoef(fitness_func.X_train.T, fitness_func.y_train)[:-1, -1])
                selected_correlations = correlations[selected_indices]
                # Remove features com MENOR correla√ß√£o
                worst_indices = selected_indices[np.argsort(selected_correlations)[:excess_count]]
            else:
                # Fallback: remove aleatoriamente
                worst_indices = np.random.choice(selected_indices, size=excess_count, replace=False)
            
            offspring[chromosome_idx, worst_indices] = 0
            
        elif selected_count == 0:
            # Adiciona feature com MAIOR correla√ß√£o
            if hasattr(fitness_func, 'X_train') and hasattr(fitness_func, 'y_train'):
                correlations = np.abs(np.corrcoef(fitness_func.X_train.T, fitness_func.y_train)[:-1, -1])
                best_feature = np.argmax(correlations)
                offspring[chromosome_idx, best_feature] = 1
            else:
                # Fallback: adiciona aleatoriamente
                random_index = np.random.randint(0, offspring.shape[1])
                offspring[chromosome_idx, random_index] = 1
        
        # üõ°Ô∏è PROTE√á√ÉO CONTRA MUTA√á√ÉO DESTRUTIVA:
        # Se a muta√ß√£o criou uma solu√ß√£o muito ruim, reverte parcialmente
        new_features = np.sum(offspring[chromosome_idx])
        if new_features > original_features + 20:  # Mudan√ßa muito dr√°stica
            # Reverte para uma vers√£o mais conservadora
            offspring[chromosome_idx] = original_solution.copy()
            # Aplica apenas 1-2 muta√ß√µes pequenas
            for _ in range(np.random.randint(1, 3)):
                random_gene = np.random.randint(0, offspring.shape[1])
                offspring[chromosome_idx, random_gene] = 1 - offspring[chromosome_idx, random_gene]
            
            # Reaplica controle de features
            if np.sum(offspring[chromosome_idx]) > MAX_FEATURES:
                selected_indices = np.where(offspring[chromosome_idx] == 1)[0]
                excess = np.sum(offspring[chromosome_idx]) - MAX_FEATURES
                to_remove = np.random.choice(selected_indices, size=excess, replace=False)
                offspring[chromosome_idx, to_remove] = 0
    
    return offspring

def custom_crossover_func(parents, offspring_size, ga_instance):
    """Fun√ß√£o de crossover customizada que respeita a restri√ß√£o de m√°ximo features configur√°vel.
    Usa sele√ß√£o por torneio para escolher os pais.
    
    Args:
        parents: Pais selecionados pelo torneio
        offspring_size: Tamanho da descend√™ncia desejada
        ga_instance: Inst√¢ncia do GA
    
    Returns:
        offspring: Descend√™ncia gerada
    """
    MAX_FEATURES = getattr(custom_crossover_func, 'max_features', 90)
    offspring = np.empty(offspring_size, dtype=int)
    
    def tournament_selection(parents, tournament_size=4):
        """Sele√ß√£o por torneio para escolher um pai."""
        # Seleciona indiv√≠duos aleat√≥rios para o torneio
        # Garante que n√£o selecione mais participantes que o n√∫mero de pais dispon√≠veis
        tournament_size = min(tournament_size, parents.shape[0])
        tournament_indices = np.random.choice(parents.shape[0], size=tournament_size, replace=False)
        tournament_parents = parents[tournament_indices]
        
        # Calcula fitness para cada participante do torneio
        best_fitness = -np.inf
        best_parent = None
        
        for parent in tournament_parents:
            fitness = fitness_func(ga_instance, parent, 0)
            if fitness > best_fitness:
                best_fitness = fitness
                best_parent = parent
        
        return best_parent
    
    for k in range(offspring_size[0]):
        # ‚úÖ Sele√ß√£o por torneio para escolher dois pais
        parent1 = tournament_selection(parents)
        parent2 = tournament_selection(parents)
        
        # Crossover de dois pontos
        crossover_point1 = np.random.randint(1, offspring_size[1])
        crossover_point2 = np.random.randint(crossover_point1, offspring_size[1])
        
        child = parent1.copy()
        child[crossover_point1:crossover_point2] = parent2[crossover_point1:crossover_point2]
        
        # ‚úÖ Garante que n√£o exceda 90 features
        selected_count = np.sum(child)
        if selected_count > MAX_FEATURES:
            # Remove features aleatoriamente at√© chegar a 90
            selected_indices = np.where(child == 1)[0]
            excess_count = selected_count - MAX_FEATURES
            indices_to_remove = np.random.choice(selected_indices, size=excess_count, replace=False)
            child[indices_to_remove] = 0
        elif selected_count == 0:
            # Garante que pelo menos uma feature seja selecionada
            random_index = np.random.randint(0, offspring_size[1])
            child[random_index] = 1
        
        offspring[k] = child
    
    return offspring

def fitness_func(ga_instance, solution, solution_idx):
    """Fun√ß√£o de fitness otimizada usando cross-validation apenas no conjunto de treino.
    
    Otimiza√ß√µes implementadas:
    - Cache de resultados para solu√ß√µes id√™nticas
    - Cross-validation reduzido durante otimiza√ß√£o
    - Penaliza√ß√£o eficiente para restri√ß√µes
    
    Args:
        ga_instance: Inst√¢ncia do GA (PyGAD).
        solution: Vetor bin√°rio de sele√ß√£o de features.
        solution_idx: √çndice da solu√ß√£o na popula√ß√£o.

    Returns:
        float: Valor de fitness.
    """
    
    selected = np.where(solution > 0.5)[0]
    if len(selected) == 0:
        return 0.001
    
    # ‚úÖ RESTRI√á√ÉO: M√°ximo de features
    MAX_FEATURES = getattr(fitness_func, 'max_features', 90)
    if len(selected) > MAX_FEATURES:
        return 0.001  # Penaliza√ß√£o severa
    
    # ‚úÖ Cache para evitar recomputa√ß√£o de solu√ß√µes id√™nticas
    solution_key = tuple(solution)
    if not hasattr(fitness_func, 'cache'):
        fitness_func.cache = {}
    
    if solution_key in fitness_func.cache:
        return fitness_func.cache[solution_key]
    
    # Penaliza√ß√£o suave para promover parcim√¥nia
    feature_penalty = getattr(fitness_func, 'feature_penalty', 0.2)
    num_features_penalty = 1.0 - (len(selected) / MAX_FEATURES) * feature_penalty
    
    try:
        # ‚úÖ Usa APENAS dados de treino para evitar data leakage
        X = fitness_func.X_train[:, selected]
        y = fitness_func.y_train
        
        # ‚úÖ Otimiza√ß√£o: CV reduzido durante busca, completo apenas no final
        cv_folds = getattr(fitness_func, 'cv_folds', 4)
        is_optimization = getattr(fitness_func, 'is_optimization', False)
        
        if is_optimization and cv_folds > 3:
            # Durante otimiza√ß√£o, usa CV reduzido para velocidade
            actual_cv = 3
        else:
            actual_cv = cv_folds
        
        model = LinearRegression()
        
        # ‚úÖ Cross-validation otimizado
        cv_r2_scores = cross_val_score(model, X, y, cv=actual_cv, scoring='r2', n_jobs=1)
        cv_neg_mse_scores = cross_val_score(model, X, y, cv=actual_cv, scoring='neg_mean_squared_error', n_jobs=1)
        
        # Converte MSE negativo para positivo
        cv_mse_scores = -cv_neg_mse_scores
        
        # Calcula m√©dias das m√©tricas de cross-validation
        mean_r2 = cv_r2_scores.mean()
        mean_mse = cv_mse_scores.mean()
        
        # ‚úÖ Fitness com bonifica√ß√£o para solu√ß√µes parcimoniosas
        if mean_r2 < 0:
            fitness_score = 0.01  # R¬≤ negativo = fitness muito baixo
        else:
            # Combina R¬≤ e inverso do MSE de forma mais equilibrada
            r2_component = max(0, mean_r2)  # Garante n√£o negativo
            mse_component = 1.0 / (mean_mse + 1e-6)
            
            # Pondera os componentes: 60% R¬≤, 40% MSE
            fitness_score = 0.6 * r2_component + 0.4 * mse_component
            
            # üéØ BONIFICA√á√ÉO ESPECIAL para solu√ß√µes com poucas features e bom R¬≤
            if len(selected) <= 30 and r2_component > 0.5:
                fitness_score *= 1.2  # +20% bonus para solu√ß√µes excelentes e simples
            elif len(selected) <= 20 and r2_component > 0.3:
                fitness_score *= 1.5  # +50% bonus para solu√ß√µes muito simples e boas
        
        # Aplica penaliza√ß√£o por n√∫mero de features (suavizada)
        # Penaliza√ß√£o mais suave para incentivar explora√ß√£o
        soft_penalty = 1.0 - (len(selected) / MAX_FEATURES) * feature_penalty * 0.5  # Reduz penaliza√ß√£o pela metade
        combined_fitness = fitness_score * soft_penalty
        
        # ‚úÖ Salva no cache (limita tamanho do cache)
        if len(fitness_func.cache) < 1000:
            fitness_func.cache[solution_key] = combined_fitness
        
        return combined_fitness
    except Exception as e:
        logger.warning(f"Erro no fitness (sol_idx={solution_idx}): {e}")
        return 0.001

def run_genetic_feature_selection(X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, 
                                  ga_params: Dict[str, Any] = None):
    """Executa o algoritmo gen√©tico para sele√ß√£o de features com par√¢metros configur√°veis."""
    # Usa par√¢metros padr√£o se n√£o fornecidos
    if ga_params is None:
        ga_params = hyperparams.get_default_params()
    
    num_features = X_train.shape[1]
    fitness_func.X_train = X_train
    fitness_func.y_train = y_train

    correlations = np.abs(np.corrcoef(X_train.T, y_train)[:-1, -1])
    initial_population = []
    
    # ‚úÖ Gera popula√ß√£o inicial com vi√©s para mais features (configur√°vel)
    MAX_FEATURES = ga_params.get("max_features", 90)
    sol_per_pop = ga_params.get("sol_per_pop", 60)
    # ‚úÖ Corrige os limites da popula√ß√£o inicial com base em MAX_FEATURES
    min_features = max(5, MAX_FEATURES // 10)  # M√≠nimo 5 ou 10% do m√°ximo
    med_features = max(min_features + 1, MAX_FEATURES // 2)  # 50% do m√°ximo
    high_features = max(med_features + 1, int(MAX_FEATURES * 0.8))  # 80% do m√°ximo
    
    logger.info(f"Faixas de features para popula√ß√£o inicial:")
    logger.info(f"  - Baixa: {min_features}-{med_features}")
    logger.info(f"  - M√©dia: {med_features}-{high_features}")
    logger.info(f"  - Alta: {high_features}-{MAX_FEATURES}")
    
    for i in range(sol_per_pop):
        # Probabilidade de sele√ß√£o baseada na correla√ß√£o
        probs = correlations / correlations.sum()
        
        if i < sol_per_pop // 3:
            # 1/3 da popula√ß√£o: muitas features
            num_features_to_select = np.random.randint(high_features, MAX_FEATURES + 1)
            num_features_to_select = min(num_features_to_select, num_features)  # N√£o pode exceder total
        elif i < 2 * sol_per_pop // 3:
            # 1/3 da popula√ß√£o: features moderadas
            num_features_to_select = np.random.randint(med_features, high_features + 1)
            num_features_to_select = min(num_features_to_select, num_features)
        else:
            # 1/3 da popula√ß√£o: poucas features
            num_features_to_select = np.random.randint(min_features, med_features + 1)
            num_features_to_select = min(num_features_to_select, num_features)
        
        solution = np.zeros(num_features, dtype=int)
        if num_features_to_select > 0:
            selected_indices = np.random.choice(num_features, size=num_features_to_select, 
                                              p=probs, replace=False)
            solution[selected_indices] = 1
        
        initial_population.append(solution)
    
    # ‚úÖ Logging das estat√≠sticas da popula√ß√£o inicial
    initial_features_counts = [np.sum(solution) for solution in initial_population]
    logger.info(f"Estat√≠sticas da popula√ß√£o inicial:")
    logger.info(f"  - M√©dia de features selecionadas: {np.mean(initial_features_counts):.1f}")
    logger.info(f"  - Min/Max features: {np.min(initial_features_counts)}/{np.max(initial_features_counts)}")
    logger.info(f"  - Solu√ß√µes com > {MAX_FEATURES} features: {sum(1 for count in initial_features_counts if count > MAX_FEATURES)}")
    
    # ‚úÖ Par√¢metros configur√°veis do GA
    num_parents_mating = max(2, ga_params.get("sol_per_pop", 60) // 2)
    
    logger.info(f"Par√¢metros do GA:")
    logger.info(f"  - Gera√ß√µes: {ga_params.get('num_generations', 150)}")
    logger.info(f"  - Popula√ß√£o: {ga_params.get('sol_per_pop', 60)}")
    logger.info(f"  - Pais para cruzamento: {num_parents_mating}")
    logger.info(f"  - Torneio K: {ga_params.get('K_tournament', 4)}")
    logger.info(f"  - Elitismo: {ga_params.get('keep_parents', 8)}")
    logger.info(f"  - M√°ximo features: {MAX_FEATURES}")
    
    # ‚úÖ Configura par√¢metros para fun√ß√µes customizadas
    custom_mutation_func.max_features = MAX_FEATURES
    custom_crossover_func.max_features = MAX_FEATURES
    fitness_func.max_features = MAX_FEATURES
    fitness_func.cv_folds = ga_params.get("cv_folds", 4)
    fitness_func.feature_penalty = ga_params.get("feature_penalty", 0.2)
    fitness_func.is_optimization = ga_params.get("is_optimization", False)
    
    # ‚úÖ Limpa cache se existir
    if hasattr(fitness_func, 'cache'):
        fitness_func.cache.clear()
    
    # ‚úÖ Configura√ß√£o do algoritmo gen√©tico com operadores customizados
    # Crit√©rios de parada otimizados para evolu√ß√£o real
    if ga_params.get("is_optimization", False):
        stop_criteria = ["reach_15", "saturate_20"]  # Parada mais agressiva durante otimiza√ß√£o
        parallel_threads = 2  # Menos threads para evitar overhead
    else:
        stop_criteria = ["reach_30", "saturate_50"]  # Permite mais evolu√ß√£o
        parallel_threads = 4
    
    # üöÄ CONFIGURA√á√ÉO AVAN√áADA PARA EVITAR ESTAGNA√á√ÉO
    def on_generation(ga_instance):
        """Callback executado a cada gera√ß√£o para monitorar diversidade e aplicar restart se necess√°rio."""
        generation = ga_instance.generations_completed
        current_fitness = ga_instance.best_solutions_fitness[-1]
        
        # üîÑ RESTART SE ESTAGNAR POR MUITO TEMPO
        if generation >= 10:  # S√≥ depois de 10 gera√ß√µes
            last_5_fitness = ga_instance.best_solutions_fitness[-5:]
            if len(set(last_5_fitness)) == 1:  # Estagna√ß√£o completa
                print(f"  [Gen {generation}] üîÑ RESTART: Estagna√ß√£o detectada, diversificando popula√ß√£o...")
                
                # Mant√©m os 20% melhores, regenera o resto
                population = ga_instance.population
                fitness_values = [fitness_func(ga_instance, sol, i) for i, sol in enumerate(population)]
                
                # Ordena por fitness
                sorted_indices = np.argsort(fitness_values)[::-1]  # Decrescente
                keep_count = max(2, len(population) // 5)  # Mant√©m 20%
                
                # Mant√©m os melhores
                new_population = population[sorted_indices[:keep_count]].copy()
                
                # Regenera o resto com mais diversidade
                correlations = np.abs(np.corrcoef(fitness_func.X_train.T, fitness_func.y_train)[:-1, -1])
                probs = correlations / correlations.sum()
                
                for i in range(keep_count, len(population)):
                    # Gera nova solu√ß√£o com diversidade for√ßada
                    solution = np.zeros(num_features, dtype=int)
                    num_features_to_select = np.random.randint(5, ga_params.get("max_features", 60) // 2)
                    
                    # Usa distribui√ß√£o mais uniforme para diversidade
                    if np.random.random() < 0.5:
                        # 50% das vezes: sele√ß√£o baseada em correla√ß√£o
                        selected_indices = np.random.choice(num_features, size=num_features_to_select, 
                                                          p=probs, replace=False)
                    else:
                        # 50% das vezes: sele√ß√£o completamente aleat√≥ria
                        selected_indices = np.random.choice(num_features, size=num_features_to_select, 
                                                          replace=False)
                    
                    solution[selected_indices] = 1
                    new_population = np.vstack([new_population, solution])
                
                ga_instance.population = new_population
        
        # üéØ Aumenta press√£o de muta√ß√£o se progresso lento
        if generation >= 5:
            recent_improvement = ga_instance.best_solutions_fitness[-1] - ga_instance.best_solutions_fitness[-5]
            if recent_improvement < 0.1:  # Progresso muito lento
                # Aumenta taxa de muta√ß√£o temporariamente
                custom_mutation_func.boost_mutation = True
            else:
                custom_mutation_func.boost_mutation = False

    ga_instance = pygad.GA(
        num_generations=ga_params.get("num_generations", 150),
        num_parents_mating=num_parents_mating,
        fitness_func=fitness_func,
        sol_per_pop=ga_params.get("sol_per_pop", 60),
        num_genes=num_features,
        gene_type=int,
        init_range_low=0,
        init_range_high=2,
        initial_population=initial_population,
        mutation_type=custom_mutation_func,  # ‚úÖ Muta√ß√£o customizada
        crossover_type=custom_crossover_func,  # ‚úÖ Crossover customizado
        gene_space=[0, 1],
        parent_selection_type="tournament",
        K_tournament=ga_params.get("K_tournament", 4),
        keep_parents=ga_params.get("keep_parents", 8),
        parallel_processing=["thread", parallel_threads],
        stop_criteria=stop_criteria,
        on_generation=on_generation  # üöÄ Callback para controle avan√ßado
    )
    
    logger.info("Iniciando algoritmo gen√©tico para sele√ß√£o de features...")
    logger.info("‚úÖ Usando sele√ß√£o por torneio (K=4) para escolha de pais no crossover")
    ga_instance.run()
    
    
    solution, solution_fitness, solution_idx = ga_instance.best_solution()
    selected = np.where(solution > 0.5)[0]
    
    # Log das features selecionadas
    selected_features = [feature_cols[i] for i in selected]
    logger.info(f"Melhor solu√ß√£o encontrada:")
    logger.info(f"Fitness: {solution_fitness}")
    logger.info(f"Features selecionadas: {selected_features}")
    logger.info(f"N√∫mero de features selecionadas: {len(selected_features)}")
    
    return selected, ga_instance

def train_and_evaluate(X_train, y_train, X_test, y_test, X_val, y_val, selected_features, feature_cols, scaler):
    """Treina o modelo final e avalia nos conjuntos de teste e valida√ß√£o.

    Args:
        X_train, y_train, X_test, y_test, X_val, y_val: Dados e targets.
        selected_features: √çndices das features selecionadas.
        feature_cols: Lista de nomes das features.
        scaler: Scaler usado.

    Returns:
        tuple: Modelo treinado, m√©tricas, predi√ß√µes e nomes das features selecionadas.
    """
    selected_names = [feature_cols[i] for i in selected_features]
    logger.info(f"Treinando modelo final com features: {selected_names}")
    model = LinearRegression()
    model.fit(X_train[:, selected_features], y_train)
    y_pred_test = model.predict(X_test[:, selected_features])
    y_pred_val = model.predict(X_val[:, selected_features])
    metrics_test = compute_metrics(y_test, y_pred_test)
    metrics_val = compute_metrics(y_val, y_pred_val)
    return model, metrics_test, metrics_val, y_pred_test, y_pred_val, selected_names

def save_results_to_csv(metrics_val: dict, metrics_test: dict, output_path: Path, selected_names):
    """Salva as m√©tricas em CSV e loga as features selecionadas.

    Args:
        metrics_val: M√©tricas de valida√ß√£o.
        metrics_test: M√©tricas de teste.
        output_path: Caminho do CSV de sa√≠da.
        selected_names: Lista de features selecionadas.
    """
    results = pd.DataFrame([
        {'Conjunto': 'Valida√ß√£o', **metrics_val},
        {'Conjunto': 'Teste', **metrics_test},
    ])
    logger.info(f"Salvando resultados em: {output_path}")
    results.to_csv(output_path, index=False)
    logger.info("Resultados salvos com sucesso.")
    logger.info(f"Features selecionadas: {selected_names}")
    logger.info('\n=== Genetic MLR Results ===')
    logger.info(f"\n{results.to_string(index=False)}")

def plot_feature_importance(model, selected_features, feature_names, output_dir):
    """Plota a import√¢ncia das features selecionadas pelo algoritmo gen√©tico.

    Args:
        model: Modelo treinado.
        selected_features: √çndices das features selecionadas.
        feature_names: Lista de nomes das features.
        output_dir: Pasta de sa√≠da.
    """
    # Calcula a import√¢ncia das features usando os coeficientes absolutos
    importance = np.abs(model.coef_)
    # Normaliza para porcentagem
    importance = 100 * importance / importance.sum()
    
    # Cria DataFrame apenas com as features selecionadas
    importance_df = pd.DataFrame({
        'Feature': [feature_names[i] for i in selected_features],
        'Importance': importance
    }).sort_values('Importance', ascending=True)
    
    plt.figure(figsize=(12, 8))
    bars = plt.barh(importance_df['Feature'], importance_df['Importance'])
    plt.xlabel('Import√¢ncia (%)')
    plt.title(f'Import√¢ncia das Features Selecionadas pelo Algoritmo Gen√©tico\nTotal de Features: {len(selected_features)}')
    plt.grid(axis='x', linestyle='--', alpha=0.7)
    
    # Adiciona valores nas barras
    for bar in bars:
        width = bar.get_width()
        plt.text(width, bar.get_y() + bar.get_height()/2, 
                f'{width:.1f}%', 
                ha='left', va='center', fontsize=9)
    
    plt.tight_layout()
    plot_path = output_dir / "feature_importance_genetic.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"Gr√°fico de import√¢ncia das features salvo em {plot_path}")

def plot_genetic_evolution(ga_instance, output_dir):
    """Plota a evolu√ß√£o do algoritmo gen√©tico.

    Args:
        ga_instance: Inst√¢ncia do GA (PyGAD).
        output_dir: Pasta de sa√≠da.
    """
    plt.figure(figsize=(12, 8))
    
    # Plota a evolu√ß√£o do melhor fitness
    plt.plot(ga_instance.best_solutions_fitness, 'b-', label='Melhor Fitness')
    
    # Calcula e plota o fitness m√©dio para cada gera√ß√£o
    mean_fitness = []
    std_fitness = []
    for generation in range(len(ga_instance.best_solutions_fitness)):
        # Obt√©m os fitness de todos os indiv√≠duos na gera√ß√£o atual
        if hasattr(ga_instance, 'population_fitness') and len(ga_instance.population_fitness) > generation:
            population_fitness = ga_instance.population_fitness[generation]
            mean_fitness.append(np.mean(population_fitness))
            std_fitness.append(np.std(population_fitness))
        else:
            # Se n√£o houver dados de popula√ß√£o, usa apenas o melhor fitness
            mean_fitness.append(ga_instance.best_solutions_fitness[generation])
            std_fitness.append(0)
    
    # Plota a m√©dia e o desvio padr√£o
    mean_fitness = np.array(mean_fitness)
    std_fitness = np.array(std_fitness)
    plt.plot(mean_fitness, 'r--', label='Fitness M√©dio')
    plt.fill_between(range(len(mean_fitness)), 
                    mean_fitness - std_fitness,
                    mean_fitness + std_fitness,
                    alpha=0.2, color='r', label='¬±1 Desvio Padr√£o')
    
    plt.xlabel('Gera√ß√£o', fontsize=12)
    plt.ylabel('Fitness', fontsize=12)
    plt.title('Evolu√ß√£o do Algoritmo Gen√©tico', fontsize=14)
    plt.legend(fontsize=10)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Adiciona estat√≠sticas
    stats_text = f'Melhor Fitness: {ga_instance.best_solutions_fitness[-1]:.3f}\n'
    stats_text += f'Fitness Final M√©dio: {mean_fitness[-1]:.3f}\n'
    stats_text += f'Desvio Padr√£o Final: {std_fitness[-1]:.3f}\n'
    stats_text += f'Gera√ß√µes: {len(ga_instance.best_solutions_fitness)}'
    
    plt.text(0.05, 0.95, stats_text,
             transform=plt.gca().transAxes,
             bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),
             fontsize=10)
    
    plt.tight_layout()
    plot_path = output_dir / "genetic_evolution.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"Gr√°fico de evolu√ß√£o gen√©tica salvo em {plot_path}")

def plot_real_vs_pred(y_true, y_pred, conjunto, output_dir):
    """Plota gr√°fico Real vs Previsto com melhorias visuais.

    Args:
        y_true: Valores reais.
        y_pred: Valores previstos.
        conjunto: Nome do conjunto.
        output_dir: Pasta de sa√≠da.
    """
    plt.figure(figsize=(10, 8))
    
    # Calcula m√©tricas para o t√≠tulo
    mse = np.mean((y_true - y_pred) ** 2)
    r2 = r2_score(y_true, y_pred)
    
    # Plota os pontos
    plt.scatter(y_true, y_pred, alpha=0.6, c='blue', label='Dados')
    
    # Linha de refer√™ncia y=x
    min_val = min(y_true.min(), y_pred.min())
    max_val = max(y_true.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='y = x')
    
    # Adiciona linha de regress√£o
    z = np.polyfit(y_true, y_pred, 1)
    p = np.poly1d(z)
    plt.plot(y_true, p(y_true), "g--", alpha=0.8, label=f'Tend√™ncia (R¬≤ = {r2:.3f})')
    
    plt.xlabel('Valor Real', fontsize=12)
    plt.ylabel('Valor Previsto', fontsize=12)
    plt.title(f'Real vs Previsto (Gen√©tico) - {conjunto}\nMSE: {mse:.3f}, R¬≤: {r2:.3f}', fontsize=14)
    plt.legend(fontsize=10)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Adiciona texto com m√©tricas
    plt.text(0.05, 0.95, f'MSE: {mse:.3f}\nR¬≤: {r2:.3f}', 
             transform=plt.gca().transAxes, 
             bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),
             fontsize=10)
    
    plt.tight_layout()
    plot_path = output_dir / f"real_vs_pred_{conjunto.lower()}_genetic.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"Gr√°fico Real vs Previsto salvo em {plot_path}")

def plot_residuals(y_true, y_pred, conjunto, output_dir):
    """Plota res√≠duos vs valor previsto com melhorias visuais.

    Args:
        y_true: Valores reais.
        y_pred: Valores previstos.
        conjunto: Nome do conjunto.
        output_dir: Pasta de sa√≠da.
    """
    residuals = y_true - y_pred
    
    plt.figure(figsize=(10, 8))
    
    # Plota os res√≠duos
    plt.scatter(y_pred, residuals, alpha=0.6, c='blue', label='Res√≠duos')
    
    # Linha de refer√™ncia y=0
    plt.axhline(y=0, color='r', linestyle='--', label='Res√≠duo = 0')
    
    # Adiciona banda de confian√ßa
    std_residuals = np.std(residuals)
    plt.axhline(y=2*std_residuals, color='gray', linestyle=':', alpha=0.5, label='¬±2œÉ')
    plt.axhline(y=-2*std_residuals, color='gray', linestyle=':', alpha=0.5)
    
    plt.xlabel('Valor Previsto', fontsize=12)
    plt.ylabel('Res√≠duo', fontsize=12)
    plt.title(f'An√°lise de Res√≠duos (Gen√©tico) - {conjunto}\nDesvio Padr√£o: {std_residuals:.3f}', fontsize=14)
    plt.legend(fontsize=10)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Adiciona estat√≠sticas dos res√≠duos
    stats_text = f'M√©dia: {np.mean(residuals):.3f}\nDesvio: {std_residuals:.3f}'
    plt.text(0.05, 0.95, stats_text,
             transform=plt.gca().transAxes,
             bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),
             fontsize=10)
    
    plt.tight_layout()
    plot_path = output_dir / f"residuos_vs_pred_{conjunto.lower()}_genetic.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"Gr√°fico de res√≠duos salvo em {plot_path}")

def plot_residuals_hist(y_true, y_pred, conjunto, output_dir):
    """Plota histograma dos res√≠duos com melhorias visuais.

    Args:
        y_true: Valores reais.
        y_pred: Valores previstos.
        conjunto: Nome do conjunto.
        output_dir: Pasta de sa√≠da.
    """
    residuals = y_true - y_pred
    
    plt.figure(figsize=(10, 8))
    
    # Plota histograma com KDE
    sns.histplot(residuals, bins=30, kde=True, stat='density', color='blue', alpha=0.6)
    
    # Adiciona curva normal para compara√ß√£o
    x = np.linspace(residuals.min(), residuals.max(), 100)
    normal = stats.norm.pdf(x, np.mean(residuals), np.std(residuals))
    plt.plot(x, normal, 'r--', label='Distribui√ß√£o Normal', alpha=0.8)
    
    # Adiciona linhas de refer√™ncia
    mean_res = np.mean(residuals)
    std_res = np.std(residuals)
    plt.axvline(mean_res, color='g', linestyle='-', label=f'M√©dia: {mean_res:.3f}')
    plt.axvline(mean_res + 2*std_res, color='r', linestyle=':', alpha=0.5, label='¬±2œÉ')
    plt.axvline(mean_res - 2*std_res, color='r', linestyle=':', alpha=0.5)
    
    plt.xlabel('Res√≠duo', fontsize=12)
    plt.ylabel('Densidade', fontsize=12)
    plt.title(f'Distribui√ß√£o dos Res√≠duos (Gen√©tico) - {conjunto}', fontsize=14)
    plt.legend(fontsize=10)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Adiciona estat√≠sticas
    stats_text = f'M√©dia: {mean_res:.3f}\nDesvio: {std_res:.3f}\nSkewness: {stats.skew(residuals):.3f}'
    plt.text(0.05, 0.95, stats_text,
             transform=plt.gca().transAxes,
             bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),
             fontsize=10)
    
    plt.tight_layout()
    plot_path = output_dir / f"hist_residuos_{conjunto.lower()}_genetic.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"Histograma dos res√≠duos salvo em {plot_path}")

def optimize_hyperparameters(X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, 
                            n_trials: int = 100, timeout: int = 3600) -> Dict[str, Any]:
    """Otimiza hiperpar√¢metros do algoritmo gen√©tico usando Optuna.
    
    Args:
        X_train, y_train, X_val, y_val, X_test, y_test: Dados de treino, valida√ß√£o e teste
        feature_cols: Lista de nomes das features
        n_trials: N√∫mero de trials do Optuna
        timeout: Timeout em segundos
        
    Returns:
        Dict com melhores par√¢metros encontrados
    """
    logger.info(f"üîç Iniciando otimiza√ß√£o de hiperpar√¢metros com {n_trials} trials")
    
    def objective(trial):
        """Fun√ß√£o objetivo para o Optuna."""
        # Define o espa√ßo de busca
        search_space = hyperparams.get_search_space()
        
        # Sugere par√¢metros para este trial
        params = {
            "num_generations": trial.suggest_int("num_generations", *search_space["num_generations"]),
            "sol_per_pop": trial.suggest_int("sol_per_pop", *search_space["sol_per_pop"]),
            "K_tournament": trial.suggest_int("K_tournament", *search_space["K_tournament"]),
            "keep_parents": trial.suggest_int("keep_parents", *search_space["keep_parents"]),
            "cv_folds": trial.suggest_int("cv_folds", *search_space["cv_folds"]),
            "max_features": trial.suggest_int("max_features", *search_space["max_features"]),
            "feature_penalty": trial.suggest_float("feature_penalty", *search_space["feature_penalty"]),
            "is_optimization": True  # ‚úÖ Ativa modo otimiza√ß√£o r√°pida
        }
        
        # Atualiza fun√ß√£o de fitness com novos par√¢metros
        fitness_func.cv_folds = params["cv_folds"]
        fitness_func.feature_penalty = params["feature_penalty"]
        fitness_func.is_optimization = True  # ‚úÖ Ativa modo otimiza√ß√£o r√°pida
        
        try:
            # Executa algoritmo gen√©tico com par√¢metros atuais
            selected_features, ga_instance = run_genetic_feature_selection(
                X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, params
            )
            
            # Treina modelo final e avalia
            model, metrics_test, metrics_val, _, _, _ = train_and_evaluate(
                X_train, y_train, X_test, y_test, X_val, y_val, 
                selected_features, feature_cols, None
            )
            
            # Fun√ß√£o objetivo: maximizar R¬≤ de valida√ß√£o e minimizar n√∫mero de features
            r2_val = metrics_val['R2']
            num_features = len(selected_features)
            
            # Score combinado: 70% R¬≤ + 30% parcim√¥nia
            parsimony_score = 1.0 - (num_features / params["max_features"])
            combined_score = 0.7 * r2_val + 0.3 * parsimony_score
            
            # Registro das m√©tricas no trial
            trial.set_user_attr("r2_val", r2_val)
            trial.set_user_attr("r2_test", metrics_test['R2'])
            trial.set_user_attr("num_features", num_features)
            trial.set_user_attr("mse_val", metrics_val['MSE'])
            trial.set_user_attr("mae_val", metrics_val['MAE'])
            
            logger.info(f"Trial {trial.number}: R¬≤={r2_val:.3f}, Features={num_features}, Score={combined_score:.3f}")
            
            return combined_score
            
        except Exception as e:
            logger.warning(f"Trial {trial.number} falhou: {e}")
            return -1.0  # Score baixo para trials que falharam
    
    # Cria estudo Optuna
    study = optuna.create_study(
        direction='maximize',
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10),
        sampler=optuna.samplers.TPESampler(seed=42)
    )
    
    # Executa otimiza√ß√£o
    study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)
    
    # Analisa resultados
    best_params = study.best_params
    best_value = study.best_value
    
    logger.info(f"üéØ Otimiza√ß√£o conclu√≠da!")
    logger.info(f"Melhor score: {best_value:.4f}")
    logger.info(f"Melhores par√¢metros: {best_params}")
    
    # Atualiza configura√ß√£o global
    hyperparams.update_best_params(best_params, best_value)
    
    # Salva resultados
    results_file = BASE_DIR / 'results' / 'best_hyperparameters.json'
    hyperparams.save_best_params(results_file)
    
    # Salva estudo completo
    study_file = BASE_DIR / 'results' / 'optuna_study.pkl'
    import pickle
    with open(study_file, 'wb') as f:
        pickle.dump(study, f)
    
    logger.info(f"Resultados salvos em: {results_file}")
    
    return best_params

def run_with_optimization(optimize: bool = True, n_trials: int = 50) -> Dict[str, Any]:
    """Executa o pipeline completo com ou sem otimiza√ß√£o de hiperpar√¢metros.
    
    Args:
        optimize: Se True, executa otimiza√ß√£o de hiperpar√¢metros
        n_trials: N√∫mero de trials para otimiza√ß√£o
        
    Returns:
        Dict com resultados finais
    """
    logger.info("üöÄ Iniciando pipeline de Regress√£o Linear M√∫ltipla com Sele√ß√£o Gen√©tica")
    
    # Carrega dados
    df, idrc_df = load_data(CALIBRATION_CSV, IDRC_CSV)
    X_train, y_train, X_test, y_test, X_val, y_val, feature_cols, scaler = prepare_datasets(df, idrc_df)
    
    best_params = None
    
    if optimize:
        # Otimiza hiperpar√¢metros
        best_params = optimize_hyperparameters(
            X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, n_trials
        )
    else:
        # Usa par√¢metros padr√£o ou carrega melhores par√¢metros salvos
        hyperparams.load_best_params(BASE_DIR / 'results' / 'best_hyperparameters.json')
        best_params = hyperparams.best_params or hyperparams.get_default_params()
        logger.info(f"Usando par√¢metros: {best_params}")
    
    # Executa algoritmo gen√©tico com melhores par√¢metros
    logger.info("üß¨ Executando algoritmo gen√©tico com melhores par√¢metros")
    selected_features, ga_instance = run_genetic_feature_selection(
        X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, best_params
    )
    
    # Treina e avalia modelo final
    model, metrics_test, metrics_val, y_pred_test, y_pred_val, selected_names = train_and_evaluate(
        X_train, y_train, X_test, y_test, X_val, y_val, selected_features, feature_cols, scaler
    )
    
    # Salva resultados
    save_results_to_csv(metrics_val, metrics_test, OUTPUT_CSV, selected_names)
    
    # Gera visualiza√ß√µes
    plot_genetic_evolution(ga_instance, OUTPUT_PLOTS)
    plot_feature_importance(model, selected_features, feature_cols, OUTPUT_PLOTS)
    plot_real_vs_pred(y_val, y_pred_val, 'Valida√ß√£o', OUTPUT_PLOTS)
    plot_residuals(y_val, y_pred_val, 'Valida√ß√£o', OUTPUT_PLOTS)
    plot_residuals_hist(y_val, y_pred_val, 'Valida√ß√£o', OUTPUT_PLOTS)
    plot_real_vs_pred(y_test, y_pred_test, 'Teste', OUTPUT_PLOTS)
    plot_residuals(y_test, y_pred_test, 'Teste', OUTPUT_PLOTS)
    plot_residuals_hist(y_test, y_pred_test, 'Teste', OUTPUT_PLOTS)
    
    logger.info("‚úÖ Pipeline conclu√≠do com sucesso!")
    
    return {
        'best_params': best_params,
        'metrics_val': metrics_val,
        'metrics_test': metrics_test,
        'selected_features': selected_names,
        'num_features': len(selected_names)
    }

def main():
    """Executa o pipeline com op√ß√µes de otimiza√ß√£o de hiperpar√¢metros."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Regress√£o Linear M√∫ltipla com Sele√ß√£o Gen√©tica de Features')
    parser.add_argument('--optimize', action='store_true', 
                       help='Executa otimiza√ß√£o de hiperpar√¢metros')
    parser.add_argument('--trials', type=int, default=50,
                       help='N√∫mero de trials para otimiza√ß√£o (padr√£o: 50)')
    parser.add_argument('--timeout', type=int, default=3600,
                       help='Timeout em segundos para otimiza√ß√£o (padr√£o: 3600)')
    
    args = parser.parse_args()
    
    if args.optimize:
        logger.info(f"üîç Modo de otimiza√ß√£o ativado - {args.trials} trials")
        results = run_with_optimization(optimize=True, n_trials=args.trials)
    else:
        logger.info("‚ö° Modo padr√£o - usando par√¢metros salvos ou padr√£o")
        results = run_with_optimization(optimize=False)
    
    # Relat√≥rio final
    logger.info("üìä RELAT√ìRIO FINAL:")
    logger.info(f"R¬≤ Valida√ß√£o: {results['metrics_val']['R2']:.4f}")
    logger.info(f"R¬≤ Teste: {results['metrics_test']['R2']:.4f}")
    logger.info(f"Features selecionadas: {results['num_features']}")
    logger.info(f"MSE Valida√ß√£o: {results['metrics_val']['MSE']:.4f}")
    logger.info(f"MAE Valida√ß√£o: {results['metrics_val']['MAE']:.4f}")
    
    return results

if __name__ == "__main__":
    main() 