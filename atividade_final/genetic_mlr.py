import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent / 'src'))
from metrics import compute_metrics
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score
import numpy as np
import pygad
from src.logging_utils import get_logger
import matplotlib.pyplot as plt
import optuna
import json
from typing import Dict, Any, Tuple
import warnings
warnings.filterwarnings('ignore')

logger = get_logger('genetic_mlr')

# --- Configura√ß√£o de Hiperpar√¢metros ---
class HyperparameterConfig:
    """Classe para gerenciar configura√ß√£o de hiperpar√¢metros do algoritmo gen√©tico."""
    
    def __init__(self):
        """Inicializa com configura√ß√£o padr√£o."""
        self.default_params = {
            "num_generations": 80,   # Reduzido para velocidade
            "sol_per_pop": 40,       # Reduzido para velocidade
            "K_tournament": 4,
            "keep_parents": 6,       # Reduzido
            "cv_folds": 4,
            "max_features": 70,      # Mais conservador
            "feature_penalty": 0.2
        }
        
        self.search_space = {
            "num_generations": (30, 150),  # Reduzido para velocidade
            "sol_per_pop": (20, 60),       # Reduzido para velocidade
            "K_tournament": (2, 6),
            "keep_parents": (2, 15),
            "cv_folds": (3, 5),            # Reduzido para velocidade
            "max_features": (30, 120),     # Mais conservador
            "feature_penalty": (0.1, 0.4)
        }
        
        self.best_params = None
        self.best_score = None
        
    def get_default_params(self) -> Dict[str, Any]:
        """Retorna par√¢metros padr√£o."""
        return self.default_params.copy()
    
    def get_search_space(self) -> Dict[str, Tuple]:
        """Retorna espa√ßo de busca para otimiza√ß√£o."""
        return self.search_space.copy()
    
    def update_best_params(self, params: Dict[str, Any], score: float):
        """Atualiza os melhores par√¢metros encontrados."""
        self.best_params = params.copy()
        self.best_score = score
        
    def save_best_params(self, filepath: Path):
        """Salva os melhores par√¢metros em arquivo JSON."""
        if self.best_params:
            with open(filepath, 'w') as f:
                json.dump({
                    'best_params': self.best_params,
                    'best_score': self.best_score
                }, f, indent=2)
                
    def load_best_params(self, filepath: Path):
        """Carrega os melhores par√¢metros de arquivo JSON."""
        if filepath.exists():
            with open(filepath, 'r') as f:
                data = json.load(f)
                self.best_params = data.get('best_params')
                self.best_score = data.get('best_score')

# Inst√¢ncia global da configura√ß√£o
hyperparams = HyperparameterConfig()

BASE_DIR = Path(__file__).resolve().parent
CALIBRATION_CSV = BASE_DIR / 'data' / 'csv_new' / 'calibration.csv'
TEST_CSV = BASE_DIR / 'data' / 'csv_new' / 'test.csv'
VALIDATION_CSV = BASE_DIR / 'data' / 'csv_new' / 'validation.csv'
IDRC_CSV = BASE_DIR / 'data' / 'csv_new' / 'idrc_validation.csv'
WL_CSV = BASE_DIR / 'data' / 'csv_new' / 'wl.csv'
OUTPUT_CSV = BASE_DIR / 'results' / 'genetic_results.csv'
OUTPUT_PLOTS = BASE_DIR / 'results' / 'genetic_plots'
OUTPUT_PLOTS.mkdir(parents=True, exist_ok=True)

def load_data(calib_path: Path, idrc_path: Path) -> tuple[pd.DataFrame, pd.DataFrame]:
    """Carrega os dados de calibra√ß√£o e IDRC.

    Args:
        calib_path (Path): Caminho para o CSV de calibra√ß√£o.
        idrc_path (Path): Caminho para o CSV de valida√ß√£o IDRC.

    Returns:
        tuple: DataFrames de calibra√ß√£o e IDRC.
    """
    logger.info(f"Carregando dados de calibra√ß√£o de: {calib_path}")
    df = pd.read_csv(calib_path)
    logger.info(f"Dados de calibra√ß√£o carregados. Shape: {df.shape}")
    logger.info(f"Carregando dados IDRC de: {idrc_path}")
    idrc_df = pd.read_csv(idrc_path)
    logger.info(f"Dados IDRC carregados. Shape: {idrc_df.shape}")
    return df, idrc_df

def prepare_datasets(df: pd.DataFrame, idrc_df: pd.DataFrame):
    """Prepara os conjuntos de treino, teste e valida√ß√£o, com imputa√ß√£o e normaliza√ß√£o.

    Args:
        df (pd.DataFrame): Dados de calibra√ß√£o.
        idrc_df (pd.DataFrame): Dados de valida√ß√£o IDRC.

    Returns:
        tuple: Arrays e listas para treino, teste, valida√ß√£o, nomes das features e scaler.
    """
    # Carrega os dados de teste e valida√ß√£o
    test_df = pd.read_csv(TEST_CSV)
    validation_df = pd.read_csv(VALIDATION_CSV)
    
    # Seleciona apenas as colunas num√©ricas (excluindo a √∫ltima coluna que √© o target)
    feature_columns = [col for col in df.columns if col != 'target']
    
    # Prepara os dados de treino (usando todos os dados de calibra√ß√£o)
    X_train = df[feature_columns]
    y_train = df['target']
    
    # Prepara os dados de teste (do arquivo test.csv)
    X_test = test_df[feature_columns]
    y_test = test_df['target']
    
    # Prepara os dados de valida√ß√£o (do arquivo validation.csv)
    X_val = validation_df[feature_columns]
    y_val = idrc_df['reference']  # Usando a coluna 'reference' do IDRC
    
    # Imputa√ß√£o de dados faltantes
    imputer = SimpleImputer(strategy='mean')
    X_train_imputed = imputer.fit_transform(X_train)
    X_test_imputed = imputer.transform(X_test)
    X_val_imputed = imputer.transform(X_val)
    
    # Normaliza√ß√£o dos dados
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_imputed)
    X_test_scaled = scaler.transform(X_test_imputed)
    X_val_scaled = scaler.transform(X_val_imputed)
    
    return X_train_scaled, y_train, X_test_scaled, y_test, X_val_scaled, y_val, feature_columns, scaler

def adaptive_mutation_func(offspring, ga_instance):
    """Muta√ß√£o adaptativa simplificada com controle de features."""
    MAX_FEATURES = getattr(adaptive_mutation_func, 'max_features', 70)
    
    for i in range(offspring.shape[0]):
        current_features = np.sum(offspring[i])
        
        # Taxa adaptativa baseada no n√∫mero de features
        mutation_rate = 0.02 if current_features <= 30 else (0.05 if current_features <= 50 else 0.15)
        
        # Aplica muta√ß√£o bit-flip
        mutations = np.random.random(offspring.shape[1]) < mutation_rate
        offspring[i] = np.where(mutations, 1 - offspring[i], offspring[i])
        
        # Controle de restri√ß√µes
        current_features = np.sum(offspring[i])
        if current_features > MAX_FEATURES:
            excess = current_features - MAX_FEATURES
            selected_indices = np.where(offspring[i] == 1)[0]
            to_remove = np.random.choice(selected_indices, size=excess, replace=False)
            offspring[i, to_remove] = 0
        elif current_features == 0:
            offspring[i, np.random.randint(0, offspring.shape[1])] = 1
    
    return offspring

# Removemos o crossover customizado para usar o nativo do PyGAD com controle de features

def fitness_func(ga_instance, solution, solution_idx):
    """Fun√ß√£o de fitness otimizada usando cross-validation apenas no conjunto de treino.
    
    Otimiza√ß√µes implementadas:
    - Cache de resultados para solu√ß√µes id√™nticas
    - Cross-validation reduzido durante otimiza√ß√£o
    - Penaliza√ß√£o eficiente para restri√ß√µes

    Args:
        ga_instance: Inst√¢ncia do GA (PyGAD).
        solution: Vetor bin√°rio de sele√ß√£o de features.
        solution_idx: √çndice da solu√ß√£o na popula√ß√£o.

    Returns:
        float: Valor de fitness.
    """
    
    selected = np.where(solution > 0.5)[0]
    if len(selected) == 0:
        return 0.001
    
    # ‚úÖ RESTRI√á√ÉO: M√°ximo de features
    MAX_FEATURES = getattr(fitness_func, 'max_features', 90)
    if len(selected) > MAX_FEATURES:
        return 0.001  # Penaliza√ß√£o severa
    
    # ‚úÖ Cache para evitar recomputa√ß√£o de solu√ß√µes id√™nticas
    solution_key = tuple(solution)
    if not hasattr(fitness_func, 'cache'):
        fitness_func.cache = {}
    
    if solution_key in fitness_func.cache:
        return fitness_func.cache[solution_key]
    
    # Penaliza√ß√£o suave para promover parcim√¥nia
    feature_penalty = getattr(fitness_func, 'feature_penalty', 0.2)
    num_features_penalty = 1.0 - (len(selected) / MAX_FEATURES) * feature_penalty
    
    try:
        # ‚úÖ Usa APENAS dados de treino para evitar data leakage
        X = fitness_func.X_train[:, selected]
        y = fitness_func.y_train
        
        # ‚úÖ Otimiza√ß√£o: CV reduzido durante busca, completo apenas no final
        cv_folds = getattr(fitness_func, 'cv_folds', 4)
        is_optimization = getattr(fitness_func, 'is_optimization', False)
        
        if is_optimization and cv_folds > 3:
            # Durante otimiza√ß√£o, usa CV reduzido para velocidade
            actual_cv = 3
        else:
            actual_cv = cv_folds
        
        model = LinearRegression()
        
        # ‚úÖ Cross-validation otimizado
        cv_r2_scores = cross_val_score(model, X, y, cv=actual_cv, scoring='r2', n_jobs=1)
        cv_neg_mse_scores = cross_val_score(model, X, y, cv=actual_cv, scoring='neg_mean_squared_error', n_jobs=1)
        
        # Converte MSE negativo para positivo
        cv_mse_scores = -cv_neg_mse_scores
        
        # Calcula m√©dias das m√©tricas de cross-validation
        mean_r2 = cv_r2_scores.mean()
        mean_mse = cv_mse_scores.mean()
        
        # ‚úÖ Fitness com bonifica√ß√£o para solu√ß√µes parcimoniosas
        if mean_r2 < 0:
            fitness_score = 0.01  # R¬≤ negativo = fitness muito baixo
        else:
            # Combina R¬≤ e inverso do MSE de forma mais equilibrada
            r2_component = max(0, mean_r2)  # Garante n√£o negativo
            mse_component = 1.0 / (mean_mse + 1e-6)
            
            # Pondera os componentes: 60% R¬≤, 40% MSE
            fitness_score = 0.6 * r2_component + 0.4 * mse_component
            
            # üéØ BONIFICA√á√ÉO ESPECIAL para solu√ß√µes com poucas features e bom R¬≤
            if len(selected) <= 30 and r2_component > 0.5:
                fitness_score *= 1.2  # +20% bonus para solu√ß√µes excelentes e simples
            elif len(selected) <= 20 and r2_component > 0.3:
                fitness_score *= 1.5  # +50% bonus para solu√ß√µes muito simples e boas
        
        # Aplica penaliza√ß√£o por n√∫mero de features (suavizada)
        # Penaliza√ß√£o mais suave para incentivar explora√ß√£o
        soft_penalty = 1.0 - (len(selected) / MAX_FEATURES) * feature_penalty * 0.5  # Reduz penaliza√ß√£o pela metade
        combined_fitness = fitness_score * soft_penalty
        
        # ‚úÖ Salva no cache (limita tamanho do cache)
        if len(fitness_func.cache) < 1000:
            fitness_func.cache[solution_key] = combined_fitness
        
        return combined_fitness
    except Exception as e:
        logger.warning(f"Erro no fitness (sol_idx={solution_idx}): {e}")
        return 0.001

def run_genetic_feature_selection(X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, 
                                  ga_params: Dict[str, Any] = None):
    """Executa o algoritmo gen√©tico para sele√ß√£o de features com par√¢metros configur√°veis."""
    # Usa par√¢metros padr√£o se n√£o fornecidos
    if ga_params is None:
        ga_params = hyperparams.get_default_params()
    
    num_features = X_train.shape[1]
    fitness_func.X_train = X_train
    fitness_func.y_train = y_train
    
    correlations = np.abs(np.corrcoef(X_train.T, y_train)[:-1, -1])
    initial_population = []
    
    # ‚úÖ Gera popula√ß√£o inicial com vi√©s para mais features (configur√°vel)
    MAX_FEATURES = ga_params.get("max_features", 90)
    sol_per_pop = ga_params.get("sol_per_pop", 60)
    # ‚úÖ Corrige os limites da popula√ß√£o inicial com base em MAX_FEATURES
    min_features = max(5, MAX_FEATURES // 10)  # M√≠nimo 5 ou 10% do m√°ximo
    med_features = max(min_features + 1, MAX_FEATURES // 2)  # 50% do m√°ximo
    high_features = max(med_features + 1, int(MAX_FEATURES * 0.8))  # 80% do m√°ximo
    
    logger.info(f"Faixas de features para popula√ß√£o inicial:")
    logger.info(f"  - Baixa: {min_features}-{med_features}")
    logger.info(f"  - M√©dia: {med_features}-{high_features}")
    logger.info(f"  - Alta: {high_features}-{MAX_FEATURES}")
    
    for i in range(sol_per_pop):
        # Probabilidade de sele√ß√£o baseada na correla√ß√£o
        probs = correlations / correlations.sum()
        
        if i < sol_per_pop // 3:
            # 1/3 da popula√ß√£o: muitas features
            num_features_to_select = np.random.randint(high_features, MAX_FEATURES + 1)
            num_features_to_select = min(num_features_to_select, num_features)  # N√£o pode exceder total
        elif i < 2 * sol_per_pop // 3:
            # 1/3 da popula√ß√£o: features moderadas
            num_features_to_select = np.random.randint(med_features, high_features + 1)
            num_features_to_select = min(num_features_to_select, num_features)
        else:
            # 1/3 da popula√ß√£o: poucas features
            num_features_to_select = np.random.randint(min_features, med_features + 1)
            num_features_to_select = min(num_features_to_select, num_features)
        
        solution = np.zeros(num_features, dtype=int)
        if num_features_to_select > 0:
            selected_indices = np.random.choice(num_features, size=num_features_to_select, 
                                              p=probs, replace=False)
            solution[selected_indices] = 1
        
        initial_population.append(solution)
    
    # ‚úÖ Logging das estat√≠sticas da popula√ß√£o inicial
    initial_features_counts = [np.sum(solution) for solution in initial_population]
    logger.info(f"Estat√≠sticas da popula√ß√£o inicial:")
    logger.info(f"  - M√©dia de features selecionadas: {np.mean(initial_features_counts):.1f}")
    logger.info(f"  - Min/Max features: {np.min(initial_features_counts)}/{np.max(initial_features_counts)}")
    logger.info(f"  - Solu√ß√µes com > {MAX_FEATURES} features: {sum(1 for count in initial_features_counts if count > MAX_FEATURES)}")
    
    # ‚úÖ Par√¢metros configur√°veis do GA
    num_parents_mating = max(2, ga_params.get("sol_per_pop", 60) // 2)
    
    logger.info(f"Par√¢metros do GA:")
    logger.info(f"  - Gera√ß√µes: {ga_params.get('num_generations', 150)}")
    logger.info(f"  - Popula√ß√£o: {ga_params.get('sol_per_pop', 60)}")
    logger.info(f"  - Pais para cruzamento: {num_parents_mating}")
    logger.info(f"  - Torneio K: {ga_params.get('K_tournament', 4)}")
    logger.info(f"  - Elitismo: {ga_params.get('keep_parents', 8)}")
    logger.info(f"  - M√°ximo features: {MAX_FEATURES}")
    
    # ‚úÖ Configura par√¢metros para fun√ß√µes
    adaptive_mutation_func.max_features = MAX_FEATURES
    fitness_func.max_features = MAX_FEATURES
    fitness_func.cv_folds = ga_params.get("cv_folds", 4)
    fitness_func.feature_penalty = ga_params.get("feature_penalty", 0.2)
    fitness_func.is_optimization = ga_params.get("is_optimization", False)
    
    # ‚úÖ Limpa cache se existir
    if hasattr(fitness_func, 'cache'):
        fitness_func.cache.clear()
    
    # ‚úÖ Configura√ß√£o do algoritmo gen√©tico com operadores customizados
    # Crit√©rios de parada otimizados para evolu√ß√£o real
    if ga_params.get("is_optimization", False):
        stop_criteria = ["reach_15", "saturate_20"]  # Parada mais agressiva durante otimiza√ß√£o
        parallel_threads = 2  # Menos threads para evitar overhead
    else:
        stop_criteria = ["reach_30", "saturate_50"]  # Permite mais evolu√ß√£o
        parallel_threads = 4
    
    # üîÑ Callback simplificado para restart autom√°tico
    def on_generation(ga_instance):
        generation = ga_instance.generations_completed
        if generation >= 10 and generation % 15 == 0:
            recent_fitness = ga_instance.best_solutions_fitness[-5:]
            if len(set(recent_fitness)) == 1:  # Estagna√ß√£o
                print(f"  [Gen {generation}] üîÑ RESTART: diversificando popula√ß√£o...")
                # Regenera 80% da popula√ß√£o mantendo 20% da elite
                pop_size = len(ga_instance.population)
                keep_count = pop_size // 5
                new_pop = create_diverse_population(num_features, pop_size - keep_count, correlations, MAX_FEATURES)
                ga_instance.population = np.vstack([ga_instance.population[:keep_count], new_pop])
    
    def create_diverse_population(num_features, pop_size, correlations, max_features):
        """Cria popula√ß√£o diversificada baseada em correla√ß√µes."""
        population = []
        probs = correlations / correlations.sum()
        for i in range(pop_size):
            solution = np.zeros(num_features, dtype=int)
            n_features = np.random.randint(5, max_features//2)
            if np.random.random() < 0.5:
                selected_indices = np.random.choice(num_features, size=n_features, p=probs, replace=False)
            else:
                selected_indices = np.random.choice(num_features, size=n_features, replace=False)
            solution[selected_indices] = 1
            population.append(solution)
        return np.array(population)

    # ‚úÖ Configura√ß√£o PyGAD simplificada
    ga_instance = pygad.GA(
        num_generations=ga_params.get("num_generations", 80),
        num_parents_mating=num_parents_mating,
        fitness_func=fitness_func,
        sol_per_pop=ga_params.get("sol_per_pop", 40),
        num_genes=num_features,
        initial_population=initial_population,
        gene_type=int,
        gene_space=[0, 1],
        
        # Operadores nativos do PyGAD
        parent_selection_type="tournament",
        K_tournament=ga_params.get("K_tournament", 4),
        crossover_type="two_points",
        crossover_probability=0.8,
        mutation_type=adaptive_mutation_func,
        
        # Elitismo nativo
        keep_elitism=ga_params.get("keep_parents", 6),
        
        # Callback e crit√©rios de parada
        on_generation=on_generation,
        stop_criteria=stop_criteria
    )
    
    logger.info("Iniciando algoritmo gen√©tico para sele√ß√£o de features...")
    logger.info("‚úÖ Usando sele√ß√£o por torneio (K=4) para escolha de pais no crossover")
    ga_instance.run()
    
    
    solution, solution_fitness, solution_idx = ga_instance.best_solution()
    selected = np.where(solution > 0.5)[0]
    
    # Log das features selecionadas
    selected_features = [feature_cols[i] for i in selected]
    logger.info(f"Melhor solu√ß√£o encontrada:")
    logger.info(f"Fitness: {solution_fitness}")
    logger.info(f"Features selecionadas: {selected_features}")
    logger.info(f"N√∫mero de features selecionadas: {len(selected_features)}")
    
    return selected, ga_instance

def train_and_evaluate(X_train, y_train, X_test, y_test, X_val, y_val, selected_features, feature_cols, scaler):
    """Treina o modelo final e avalia nos conjuntos de teste e valida√ß√£o.

    Args:
        X_train, y_train, X_test, y_test, X_val, y_val: Dados e targets.
        selected_features: √çndices das features selecionadas.
        feature_cols: Lista de nomes das features.
        scaler: Scaler usado.

    Returns:
        tuple: Modelo treinado, m√©tricas, predi√ß√µes e nomes das features selecionadas.
    """
    selected_names = [feature_cols[i] for i in selected_features]
    logger.info(f"Treinando modelo final com features: {selected_names}")
    model = LinearRegression()
    model.fit(X_train[:, selected_features], y_train)
    y_pred_test = model.predict(X_test[:, selected_features])
    y_pred_val = model.predict(X_val[:, selected_features])
    metrics_test = compute_metrics(y_test, y_pred_test)
    metrics_val = compute_metrics(y_val, y_pred_val)
    return model, metrics_test, metrics_val, y_pred_test, y_pred_val, selected_names

def save_results_to_csv(metrics_val: dict, metrics_test: dict, output_path: Path, selected_names):
    """Salva as m√©tricas em CSV e loga as features selecionadas.

    Args:
        metrics_val: M√©tricas de valida√ß√£o.
        metrics_test: M√©tricas de teste.
        output_path: Caminho do CSV de sa√≠da.
        selected_names: Lista de features selecionadas.
    """
    results = pd.DataFrame([
        {'Conjunto': 'Valida√ß√£o', **metrics_val},
        {'Conjunto': 'Teste', **metrics_test},
    ])
    logger.info(f"Salvando resultados em: {output_path}")
    results.to_csv(output_path, index=False)
    logger.info("Resultados salvos com sucesso.")
    logger.info(f"Features selecionadas: {selected_names}")
    logger.info('\n=== Genetic MLR Results ===')
    logger.info(f"\n{results.to_string(index=False)}")

def plot_feature_importance(model, selected_features, feature_names, output_dir):
    """Plota a import√¢ncia das features selecionadas pelo algoritmo gen√©tico.

    Args:
        model: Modelo treinado.
        selected_features: √çndices das features selecionadas.
        feature_names: Lista de nomes das features.
        output_dir: Pasta de sa√≠da.
    """
    # Calcula a import√¢ncia das features usando os coeficientes absolutos
    importance = np.abs(model.coef_)
    # Normaliza para porcentagem
    importance = 100 * importance / importance.sum()
    
    # Cria DataFrame apenas com as features selecionadas
    importance_df = pd.DataFrame({
        'Feature': [feature_names[i] for i in selected_features],
        'Importance': importance
    }).sort_values('Importance', ascending=True)
    
    plt.figure(figsize=(12, 8))
    bars = plt.barh(importance_df['Feature'], importance_df['Importance'])
    plt.xlabel('Import√¢ncia (%)')
    plt.title(f'Import√¢ncia das Features Selecionadas pelo Algoritmo Gen√©tico\nTotal de Features: {len(selected_features)}')
    plt.grid(axis='x', linestyle='--', alpha=0.7)
    
    # Adiciona valores nas barras
    for bar in bars:
        width = bar.get_width()
        plt.text(width, bar.get_y() + bar.get_height()/2, 
                f'{width:.1f}%', 
                ha='left', va='center', fontsize=9)
    
    plt.tight_layout()
    plot_path = output_dir / "feature_importance_genetic.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"Gr√°fico de import√¢ncia das features salvo em {plot_path}")

def create_plots(ga_instance, model, selected_features, feature_cols, y_val, y_pred_val, y_test, y_pred_test):
    """Cria todos os gr√°ficos necess√°rios de forma simplificada."""
    
    # 1. Evolu√ß√£o gen√©tica
    plt.figure(figsize=(10, 6))
    plt.plot(ga_instance.best_solutions_fitness, 'b-', linewidth=2, label='Melhor Fitness')
    plt.xlabel('Gera√ß√£o')
    plt.ylabel('Fitness')
    plt.title('Evolu√ß√£o do Algoritmo Gen√©tico')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.savefig(OUTPUT_PLOTS / "genetic_evolution.png", dpi=300)
    plt.close()
    
    # 2. Import√¢ncia das features
    importance = 100 * np.abs(model.coef_) / np.abs(model.coef_).sum()
    feature_names = [feature_cols[i] for i in selected_features]
    
    plt.figure(figsize=(10, 6))
    sorted_idx = np.argsort(importance)
    plt.barh(range(len(importance)), importance[sorted_idx])
    plt.yticks(range(len(importance)), [feature_names[i] for i in sorted_idx])
    plt.xlabel('Import√¢ncia (%)')
    plt.title(f'Import√¢ncia das Features (Total: {len(selected_features)})')
    plt.tight_layout()
    plt.savefig(OUTPUT_PLOTS / "feature_importance_genetic.png", dpi=300)
    plt.close()
    
    # 3. Real vs Previsto (fun√ß√£o simplificada)
    for y_true, y_pred, nome in [(y_val, y_pred_val, 'valida√ß√£o'), (y_test, y_pred_test, 'teste')]:
        plt.figure(figsize=(8, 6))
        plt.scatter(y_true, y_pred, alpha=0.6)
        min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='y = x')
        plt.xlabel('Valor Real')
        plt.ylabel('Valor Previsto')
        plt.title(f'Real vs Previsto - {nome.title()}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(OUTPUT_PLOTS / f"real_vs_pred_{nome}_genetic.png", dpi=300)
        plt.close()
        
        # Histograma dos res√≠duos
        residuals = y_true - y_pred
        plt.figure(figsize=(8, 6))
        plt.hist(residuals, bins=20, alpha=0.7, edgecolor='black')
        plt.xlabel('Res√≠duo')
        plt.ylabel('Frequ√™ncia')
        plt.title(f'Distribui√ß√£o dos Res√≠duos - {nome.title()}')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(OUTPUT_PLOTS / f"hist_residuos_{nome}_genetic.png", dpi=300)
        plt.close()
    
    logger.info(f"Gr√°ficos salvos em {OUTPUT_PLOTS}")

def optimize_hyperparameters(X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, 
                            n_trials: int = 100, timeout: int = 3600) -> Dict[str, Any]:
    """Otimiza hiperpar√¢metros do algoritmo gen√©tico usando Optuna.
    
    Args:
        X_train, y_train, X_val, y_val, X_test, y_test: Dados de treino, valida√ß√£o e teste
        feature_cols: Lista de nomes das features
        n_trials: N√∫mero de trials do Optuna
        timeout: Timeout em segundos
        
    Returns:
        Dict com melhores par√¢metros encontrados
    """
    logger.info(f"üîç Iniciando otimiza√ß√£o de hiperpar√¢metros com {n_trials} trials")
    
    def objective(trial):
        """Fun√ß√£o objetivo para o Optuna."""
        # Define o espa√ßo de busca
        search_space = hyperparams.get_search_space()
        
        # Sugere par√¢metros para este trial
        params = {
            "num_generations": trial.suggest_int("num_generations", *search_space["num_generations"]),
            "sol_per_pop": trial.suggest_int("sol_per_pop", *search_space["sol_per_pop"]),
            "K_tournament": trial.suggest_int("K_tournament", *search_space["K_tournament"]),
            "keep_parents": trial.suggest_int("keep_parents", *search_space["keep_parents"]),
            "cv_folds": trial.suggest_int("cv_folds", *search_space["cv_folds"]),
            "max_features": trial.suggest_int("max_features", *search_space["max_features"]),
            "feature_penalty": trial.suggest_float("feature_penalty", *search_space["feature_penalty"]),
            "is_optimization": True  # ‚úÖ Ativa modo otimiza√ß√£o r√°pida
        }
        
        # Atualiza fun√ß√£o de fitness com novos par√¢metros
        fitness_func.cv_folds = params["cv_folds"]
        fitness_func.feature_penalty = params["feature_penalty"]
        fitness_func.is_optimization = True  # ‚úÖ Ativa modo otimiza√ß√£o r√°pida
        
        try:
            # Executa algoritmo gen√©tico com par√¢metros atuais
            selected_features, ga_instance = run_genetic_feature_selection(
                X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, params
            )
            
            # Treina modelo final e avalia
            model, metrics_test, metrics_val, _, _, _ = train_and_evaluate(
                X_train, y_train, X_test, y_test, X_val, y_val, 
                selected_features, feature_cols, None
            )
            
            # Fun√ß√£o objetivo: maximizar R¬≤ de valida√ß√£o e minimizar n√∫mero de features
            r2_val = metrics_val['R2']
            num_features = len(selected_features)
            
            # Score combinado: 70% R¬≤ + 30% parcim√¥nia
            parsimony_score = 1.0 - (num_features / params["max_features"])
            combined_score = 0.7 * r2_val + 0.3 * parsimony_score
            
            # Registro das m√©tricas no trial
            trial.set_user_attr("r2_val", r2_val)
            trial.set_user_attr("r2_test", metrics_test['R2'])
            trial.set_user_attr("num_features", num_features)
            trial.set_user_attr("mse_val", metrics_val['MSE'])
            trial.set_user_attr("mae_val", metrics_val['MAE'])
            
            logger.info(f"Trial {trial.number}: R¬≤={r2_val:.3f}, Features={num_features}, Score={combined_score:.3f}")
            
            return combined_score
            
        except Exception as e:
            logger.warning(f"Trial {trial.number} falhou: {e}")
            return -1.0  # Score baixo para trials que falharam
    
    # Cria estudo Optuna
    study = optuna.create_study(
        direction='maximize',
        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10),
        sampler=optuna.samplers.TPESampler(seed=42)
    )
    
    # Executa otimiza√ß√£o
    study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)
    
    # Analisa resultados
    best_params = study.best_params
    best_value = study.best_value
    
    logger.info(f"üéØ Otimiza√ß√£o conclu√≠da!")
    logger.info(f"Melhor score: {best_value:.4f}")
    logger.info(f"Melhores par√¢metros: {best_params}")
    
    # Atualiza configura√ß√£o global
    hyperparams.update_best_params(best_params, best_value)
    
    # Salva resultados
    results_file = BASE_DIR / 'results' / 'best_hyperparameters.json'
    hyperparams.save_best_params(results_file)
    
    # Salva estudo completo
    study_file = BASE_DIR / 'results' / 'optuna_study.pkl'
    import pickle
    with open(study_file, 'wb') as f:
        pickle.dump(study, f)
    
    logger.info(f"Resultados salvos em: {results_file}")
    
    return best_params

def run_with_optimization(optimize: bool = True, n_trials: int = 50) -> Dict[str, Any]:
    """Executa o pipeline completo com ou sem otimiza√ß√£o de hiperpar√¢metros.
    
    Args:
        optimize: Se True, executa otimiza√ß√£o de hiperpar√¢metros
        n_trials: N√∫mero de trials para otimiza√ß√£o
        
    Returns:
        Dict com resultados finais
    """
    logger.info("üöÄ Iniciando pipeline de Regress√£o Linear M√∫ltipla com Sele√ß√£o Gen√©tica")
    
    # Carrega dados
    df, idrc_df = load_data(CALIBRATION_CSV, IDRC_CSV)
    X_train, y_train, X_test, y_test, X_val, y_val, feature_cols, scaler = prepare_datasets(df, idrc_df)
    
    best_params = None
    
    if optimize:
        # Otimiza hiperpar√¢metros
        best_params = optimize_hyperparameters(
            X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, n_trials
        )
    else:
        # Usa par√¢metros padr√£o ou carrega melhores par√¢metros salvos
        hyperparams.load_best_params(BASE_DIR / 'results' / 'best_hyperparameters.json')
        best_params = hyperparams.best_params or hyperparams.get_default_params()
        logger.info(f"Usando par√¢metros: {best_params}")
    
    # Executa algoritmo gen√©tico com melhores par√¢metros
    logger.info("üß¨ Executando algoritmo gen√©tico com melhores par√¢metros")
    selected_features, ga_instance = run_genetic_feature_selection(
        X_train, y_train, X_val, y_val, X_test, y_test, feature_cols, best_params
    )
    
    # Treina e avalia modelo final
    model, metrics_test, metrics_val, y_pred_test, y_pred_val, selected_names = train_and_evaluate(
        X_train, y_train, X_test, y_test, X_val, y_val, selected_features, feature_cols, scaler
    )
    
    # Salva resultados
    save_results_to_csv(metrics_val, metrics_test, OUTPUT_CSV, selected_names)
    
    # Gera visualiza√ß√µes simplificadas
    create_plots(ga_instance, model, selected_features, feature_cols, y_val, y_pred_val, y_test, y_pred_test)
    
    logger.info("‚úÖ Pipeline conclu√≠do com sucesso!")
    
    return {
        'best_params': best_params,
        'metrics_val': metrics_val,
        'metrics_test': metrics_test,
        'selected_features': selected_names,
        'num_features': len(selected_names)
    }

def main():
    """Executa o pipeline com op√ß√µes de otimiza√ß√£o de hiperpar√¢metros."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Regress√£o Linear M√∫ltipla com Sele√ß√£o Gen√©tica de Features')
    parser.add_argument('--optimize', action='store_true', 
                       help='Executa otimiza√ß√£o de hiperpar√¢metros')
    parser.add_argument('--trials', type=int, default=50,
                       help='N√∫mero de trials para otimiza√ß√£o (padr√£o: 50)')
    parser.add_argument('--timeout', type=int, default=3600,
                       help='Timeout em segundos para otimiza√ß√£o (padr√£o: 3600)')
    
    args = parser.parse_args()
    
    if args.optimize:
        logger.info(f"üîç Modo de otimiza√ß√£o ativado - {args.trials} trials")
        results = run_with_optimization(optimize=True, n_trials=args.trials)
    else:
        logger.info("‚ö° Modo padr√£o - usando par√¢metros salvos ou padr√£o")
        results = run_with_optimization(optimize=False)
    
    # Relat√≥rio final
    logger.info("üìä RELAT√ìRIO FINAL:")
    logger.info(f"R¬≤ Valida√ß√£o: {results['metrics_val']['R2']:.4f}")
    logger.info(f"R¬≤ Teste: {results['metrics_test']['R2']:.4f}")
    logger.info(f"Features selecionadas: {results['num_features']}")
    logger.info(f"MSE Valida√ß√£o: {results['metrics_val']['MSE']:.4f}")
    logger.info(f"MAE Valida√ß√£o: {results['metrics_val']['MAE']:.4f}")
    
    return results

if __name__ == "__main__":
    main() 